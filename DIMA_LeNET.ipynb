{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "# import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BW=6\n",
    "BX=6\n",
    "VBLMAX = 0.8 \n",
    "T0 = 100e-12\n",
    "kn = 220e-6\n",
    "Vt = 0.4\n",
    "alpha = 1.8\n",
    "CBL = 270e-15\n",
    "VWL = 0.9\n",
    "Icell = kn*np.power(VWL-Vt,alpha) # Ideal cell current of the discharge path\n",
    "delta_VBL_LSB = T0*Icell/CBL #The voltage difference on VBL created by the LSB\n",
    "kclip = VBLMAX/delta_VBL_LSB \n",
    "#kclip = 10000+VBLMAX/delta_VBL_LSB\n",
    "sigma_Vt = 23.8e-3\n",
    "sigma_D = alpha*sigma_Vt/(VWL-Vt)\n",
    "\n",
    "def quantizeWeight(W,BW):\n",
    "    W = torch.min(W,(1.0-(2**(-(BW-1.0))))*torch.ones_like(W))\n",
    "    Wbs = []\n",
    "    Wbi = torch.lt(W,torch.zeros_like(W)).float()\n",
    "    Wbs.append(Wbi)\n",
    "    W = (W + Wbi)\n",
    "    for i in range(BW-1):\n",
    "        Wbi = torch.ge(W,0.5*torch.ones_like(W)).float()\n",
    "        Wbs.append(Wbi)\n",
    "        W = 2.0*W - Wbi\n",
    "    carry = torch.ge(W,0.5*torch.ones_like(W)).float()\n",
    "    for i in range(BW):#-1):\n",
    "        j = BW-1-i\n",
    "        Wbs[j] = Wbs[j]+carry\n",
    "        carry = torch.gt(Wbs[j],1.5*torch.ones_like(Wbs[j])).float()\n",
    "        Wbs[j] = Wbs[j]*torch.ne(Wbs[j],2.0*torch.ones_like(Wbs[j]))\n",
    "    return Wbs\n",
    "\n",
    "def reconstructWeight(Wbs,BW):\n",
    "    W = torch.zeros_like(Wbs[0])\n",
    "    for j in range(BW):\n",
    "        multiplier = (0.5)**j\n",
    "        if (j == 0):\n",
    "            multiplier = -1.0\n",
    "        W += Wbs[j] * multiplier\n",
    "    return W\n",
    "\n",
    "class DIMAConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sigma_D = 0,\n",
    "        layer_index = 0,\n",
    "        *kargs,\n",
    "        **kwargs\n",
    "    ):\n",
    "#         self.groups = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "        super(DIMAConv2d, self).__init__(*kargs,**kwargs)\n",
    "#         self.groups = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "#         self.output_h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "#         self.output_w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "        self.layer_index = layer_index\n",
    "        self.noise = np.random.normal(0,sigma_D,(BW,self.weight.size()[0],self.weight.size()[1]\n",
    "                                                 ,self.weight.size()[2],self.weight.size()[3]))\n",
    "        \n",
    "    def quantize_activations(self,input):\n",
    "        if(self.layer_index != 0):\n",
    "            input = torch.clamp(input,0,6) / 6\n",
    "            input = 6 * torch.min(self.round_f(input*(2**BX))*(2**(-BX)) ,(1.0-(2**(-BX)))*torch.ones_like(input))\n",
    "        else:\n",
    "            input = torch.min(self.round_f(input*(2**BX))*(2**(-BX)) ,(1.0-(2**(-BX)))*torch.ones_like(input))\n",
    "        return input\n",
    "    \n",
    "    def quantize_outputs(self,output):\n",
    "        output = torch.clamp(output,-6,6)\n",
    "        output = torch.min(self.round_f((output/6)*(2**(BW-1.0)))*(2.0**(1.0-BW)),(1.0-(2.0**(1.0-BW)))*torch.ones_like(output))\n",
    "        output = output * 6\n",
    "        return output \n",
    "               \n",
    "    def round_f(self, x): #rounds a number to the nearest integer with STE for gradients\n",
    "        x_r = torch.round(x)\n",
    "        x_g = x\n",
    "        return (x_r - x_g).detach() + x_g\n",
    "    \n",
    "    def quantize_weights(self):\n",
    "        weight_q = quantizeWeight(self.weight.data,BW)\n",
    "        for b in range(BW-1):\n",
    "            weight_q[b+1] = weight_q[b+1]*(1+self.noise[b])\n",
    "        weight = reconstructWeight(weight_q,BW)\n",
    "        Wmax = kclip*np.power(2.0,-(BW-1))\n",
    "        weight = torch.clamp(weight,-Wmax,Wmax)\n",
    "        return (weight - self.weight).detach() + self.weight\n",
    "        \n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        weight = self.quantize_weights()\n",
    "        input = self.quantize_activations(input)\n",
    "        output_h = int((((input.size()[2] + (2 * self.padding[0]) - \n",
    "                            ( self.dilation[0] * (self.kernel_size[0] - 1) ) - 1 )/ self.stride[0]) + 1) // 1)\n",
    "        output_w = int((((input.size()[3] + (2 * self.padding[1]) - \n",
    "                            ( self.dilation[1] * (self.kernel_size[1] - 1) ) - 1 )/ self.stride[1]) + 1) // 1)\n",
    "        \n",
    "        if(self.kernel_size[0]*self.kernel_size[1]*self.in_channels > 256):\n",
    "            weights = []\n",
    "            inputs = []\n",
    "            val = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "            coeff = self.in_channels // val\n",
    "            for i in range (val):\n",
    "                if(i != val-1):\n",
    "                    temp_weight = torch.zeros_like(weight)\n",
    "                    temp_input = torch.zeros_like(input)\n",
    "                    temp_weight[:,i*coeff:(i+1)*coeff,:,:] = weight[:,i*coeff:(i+1)*coeff,:,:]\n",
    "                    weights.append(temp_weight)\n",
    "                    temp_input[:,i*coeff:(i+1)*coeff,:,:] = input[:,i*coeff:(i+1)*coeff,:,:]\n",
    "                    inputs.append(temp_input)\n",
    "                else:\n",
    "                    temp_weight = torch.zeros_like(weight)\n",
    "                    temp_input = torch.zeros_like(input)\n",
    "                    temp_weight[:,i*coeff:,:,:] = weight[:,i*coeff:,:,:]\n",
    "                    weights.append(temp_weight)\n",
    "                    temp_input[:,i*coeff:,:,:] = input[:,i*coeff:,:,:]\n",
    "                    inputs.append(temp_input)\n",
    "            output = torch.zeros((input.size()[0],self.out_channels,output_h,output_w))\n",
    "            for i in range (val):\n",
    "                out = self._conv_forward(inputs[i],weights[i])\n",
    "                output += self.quantize_outputs(out)  \n",
    "#                 output += out\n",
    "            \n",
    "#         self.groups = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "#         print(self.groups)\n",
    "#         if(self.groups != 1):\n",
    "#             weight = weight.reshape(self.groups,self.out_channels//self.groups, self.in_channels, weight.size()[2], weight.size()[3])\n",
    "#         output =  self._conv_forward(input, weight)\n",
    "#         output = self.quantize_outputs(output)\n",
    "#         print(output.size())\n",
    "#         if(self.groups != 1):\n",
    "#             output = torch.sum(output, 2)\n",
    "        else:\n",
    "            output = self._conv_forward(input, weight)\n",
    "            output = self.quantize_outputs(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6df835d63b0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./mnist_LeNET.pth'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1,bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1,bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1,bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=120, out_features=84,bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=n_classes,bias=False),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs\n",
    "    \n",
    "# torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# model = LeNet5(N_CLASSES).to(DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "# model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS, DEVICE)\n",
    "\n",
    "def get_datasets(*args, **kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor()\n",
    "#             transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(train=True, transform=transform, *args, **kwargs)\n",
    "    testset = torchvision.datasets.MNIST(train=False, transform=transform, *args, **kwargs)\n",
    "    return trainset, testset\n",
    "\n",
    "def get_dataloaders(trainset, testset, batch_size=100, num_worker=4):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_worker)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_worker)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "LeNET = LeNet5(10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_ = 0.002\n",
    "optimizer = torch.optim.SGD(LeNET.parameters(), lr=lr_)\n",
    "\n",
    "\n",
    "trainset, testset = get_datasets(root='./data', download=True)\n",
    "trainloader, testloader = get_dataloaders(trainset, testset, batch_size=100, num_worker=16)\n",
    "loss_ = np.zeros(15)\n",
    "gradient = np.zeros((4,15))\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = LeNET(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    \n",
    "print('Finished Training')\n",
    "\n",
    "PATH = './mnist_LeNET.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "LeNET.load_state_dict(torch.load(PATH), strict=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs, _ = LeNET(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 56 %\n"
     ]
    }
   ],
   "source": [
    "class LeNet5_DIMA(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(LeNet5_DIMA, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "            DIMAConv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1,bias=False, sigma_D = sigma_D, layer_index = 0),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            DIMAConv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1,bias=False,sigma_D = sigma_D, layer_index = 1),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            DIMAConv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1,bias=False, sigma_D = sigma_D, layer_index = 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=120, out_features=84,bias=False),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=84, out_features=n_classes,bias=False),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return logits, probs\n",
    "\n",
    "LeNET_DIMA = LeNet5_DIMA(10)\n",
    "LeNET_DIMA.load_state_dict(torch.load(PATH), strict=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs, _ = LeNET_DIMA(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "lr_ = 0.0005\n",
    "optimizer = torch.optim.SGD(LeNET_DIMA.parameters(), lr=lr_)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = LeNET_DIMA(inputs)\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    \n",
    "print('Finished Training')\n",
    "\n",
    "PATH_ = './mnist_LeNET_dima.pth'\n",
    "torch.save(LeNET_DIMA.state_dict(), PATH_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs, _ = LeNET_DIMA(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "# time_.append(time.time() - start_time)\n",
    "# acc.append(100 * correct / total)\n",
    "#     print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#         100 * correct / total))\n",
    "\n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "# print('Time:',np.average(np.array(time_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
