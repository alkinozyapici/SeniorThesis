{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "# import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "# import torch\n",
    "# from torch import Tensor\n",
    "# from torch.nn.parameter import Parameter\n",
    "# from torch.nn import init\n",
    "# from torch.nn import Module\n",
    "# # from torch.nn import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "\n",
    "# from torch.nn import common_types\n",
    "# from torch.nn import _size_1_t, _size_2_t, _size_3_t\n",
    "# from typing import Optional, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BW=6\n",
    "BX=6\n",
    "VBLMAX = 0.8 \n",
    "T0 = 100e-12\n",
    "kn = 220e-6\n",
    "Vt = 0.4\n",
    "alpha = 1.8\n",
    "CBL = 270e-15\n",
    "VWL = 0.9\n",
    "Icell = kn*np.power(VWL-Vt,alpha) # Ideal cell current of the discharge path\n",
    "delta_VBL_LSB = T0*Icell/CBL #The voltage difference on VBL created by the LSB\n",
    "kclip = VBLMAX/delta_VBL_LSB \n",
    "#kclip = 10000+VBLMAX/delta_VBL_LSB\n",
    "sigma_Vt = 23.8e-3\n",
    "sigma_D = alpha*sigma_Vt/(VWL-Vt)\n",
    "\n",
    "def quantizeWeight(W,BW):\n",
    "    W = torch.min(W,(1.0-(2**(-(BW-1.0))))*torch.ones_like(W))\n",
    "    Wbs = []\n",
    "    Wbi = torch.lt(W,torch.zeros_like(W)).float()\n",
    "    Wbs.append(Wbi)\n",
    "    W = (W + Wbi)\n",
    "    for i in range(BW-1):\n",
    "        Wbi = torch.ge(W,0.5*torch.ones_like(W)).float()\n",
    "        Wbs.append(Wbi)\n",
    "        W = 2.0*W - Wbi\n",
    "    carry = torch.ge(W,0.5*torch.ones_like(W)).float()\n",
    "    for i in range(BW):#-1):\n",
    "        j = BW-1-i\n",
    "        Wbs[j] = Wbs[j]+carry\n",
    "        carry = torch.gt(Wbs[j],1.5*torch.ones_like(Wbs[j])).float()\n",
    "        Wbs[j] = Wbs[j]*torch.ne(Wbs[j],2.0*torch.ones_like(Wbs[j]))\n",
    "    return Wbs\n",
    "\n",
    "def reconstructWeight(Wbs,BW):\n",
    "    W = torch.zeros_like(Wbs[0])\n",
    "    for j in range(BW):\n",
    "        multiplier = (0.5)**j\n",
    "        if (j == 0):\n",
    "            multiplier = -1.0\n",
    "        W += Wbs[j] * multiplier\n",
    "    return W\n",
    "\n",
    "class DIMAConv2d(nn.Conv2d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sigma_D = 0,\n",
    "        layer_index = 0,\n",
    "        *kargs,\n",
    "        **kwargs\n",
    "    ):\n",
    "#         self.groups = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "        super(DIMAConv2d, self).__init__(*kargs,**kwargs)\n",
    "#         self.groups = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "#         self.output_h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "#         self.output_w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "        self.layer_index = layer_index\n",
    "        self.noise = np.random.normal(0,sigma_D,(BW,self.weight.size()[0],self.weight.size()[1]\n",
    "                                                 ,self.weight.size()[2],self.weight.size()[3]))\n",
    "        \n",
    "    def quantize_activations(self,input):\n",
    "        if(self.layer_index != 0):\n",
    "            input = torch.clamp(input,0,6) / 6\n",
    "            input = 6 * torch.min(self.round_f(input*(2**BX))*(2**(-BX)) ,(1.0-(2**(-BX)))*torch.ones_like(input))\n",
    "        else:\n",
    "            input = torch.min(self.round_f(input*(2**BX))*(2**(-BX)) ,(1.0-(2**(-BX)))*torch.ones_like(input))\n",
    "        return input\n",
    "    \n",
    "    def quantize_outputs(self,output):\n",
    "        output = torch.clamp(output,-6,6)\n",
    "        output = torch.min(self.round_f((output/6)*(2**(BW-1.0)))*(2.0**(1.0-BW)),(1.0-(2.0**(1.0-BW)))*torch.ones_like(output))\n",
    "        output = output * 6\n",
    "        return output \n",
    "               \n",
    "    def round_f(self, x): #rounds a number to the nearest integer with STE for gradients\n",
    "        x_r = torch.round(x)\n",
    "        x_g = x\n",
    "        return (x_r - x_g).detach() + x_g\n",
    "    \n",
    "    def quantize_weights(self):\n",
    "        weight_q = quantizeWeight(self.weight.data,BW)\n",
    "        for b in range(BW-1):\n",
    "            weight_q[b+1] = weight_q[b+1]*(1+self.noise[b])\n",
    "        weight = reconstructWeight(weight_q,BW)\n",
    "        Wmax = kclip*np.power(2.0,-(BW-1))\n",
    "        weight = torch.clamp(weight,-Wmax,Wmax)\n",
    "        return (weight - self.weight).detach() + self.weight\n",
    "        \n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "#             print(1)\n",
    "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            _pair(0), self.dilation, self.groups)\n",
    "#         print(2)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        weight = self.quantize_weights()\n",
    "        input = self.quantize_activations(input)\n",
    "#         print(weight.size())\n",
    "        output_h = int((((input.size()[2] + (2 * self.padding[0]) - \n",
    "                            ( self.dilation[0] * (self.kernel_size[0] - 1) ) - 1 )/ self.stride[0]) + 1) // 1)\n",
    "        output_w = int((((input.size()[3] + (2 * self.padding[1]) - \n",
    "                            ( self.dilation[1] * (self.kernel_size[1] - 1) ) - 1 )/ self.stride[1]) + 1) // 1)\n",
    "        \n",
    "        if(self.kernel_size[0]*self.kernel_size[1]*self.in_channels > 256):\n",
    "            print(1)\n",
    "            weights = []\n",
    "            inputs = []\n",
    "            val = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "            coeff = self.in_channels // val\n",
    "            \n",
    "#             rem = self.in_channels % val\n",
    "#             print(val, coeff)\n",
    "            for i in range (val):\n",
    "                if(i != val-1):\n",
    "                    temp_weight = torch.zeros_like(weight)\n",
    "                    temp_input = torch.zeros_like(input)\n",
    "                    temp_weight[:,i*coeff:(i+1)*coeff,:,:] = weight[:,i*coeff:(i+1)*coeff,:,:]\n",
    "                    weights.append(temp_weight)\n",
    "#                     print(weight[:,i*coeff:(i+1)*coeff,:,:].size())\n",
    "                    temp_input[:,i*coeff:(i+1)*coeff,:,:] = input[:,i*coeff:(i+1)*coeff,:,:]\n",
    "                    inputs.append(temp_input)\n",
    "#                     print(input[:,i*coeff:(i+1)*coeff,:,:].size())\n",
    "                else:\n",
    "#                     temp_weight = torch.zeros(weight.size()[0],coeff,weight.size()[2],weight.size()[3])\n",
    "#                     temp_input = torch.zeros(input.size()[0],coeff,input.size()[2],input.size()[3])\n",
    "#                     temp_input[:,:input.size()[1]-i*coeff,:,:] = input[:,i*coeff:,:,:]\n",
    "#                     print(temp_weight.size())\n",
    "#                     print(weight[:,i*coeff:,:,:].size())\n",
    "                    temp_weight = torch.zeros_like(weight)\n",
    "                    temp_input = torch.zeros_like(input)\n",
    "                    temp_weight[:,i*coeff:,:,:] = weight[:,i*coeff:,:,:]\n",
    "                    weights.append(temp_weight)\n",
    "                    temp_input[:,i*coeff:,:,:] = input[:,i*coeff:,:,:]\n",
    "                    inputs.append(temp_input)\n",
    "            output = torch.zeros((input.size()[0],self.out_channels,output_h,output_w))\n",
    "#             prin(output.size())\n",
    "            for i in range (val):\n",
    "#                 output = self._conv_forward(inputs[i],weights[i])\n",
    "#                 print(out.size())\n",
    "#                 output += self.quantize_outputs(out)\n",
    "                if(i != 0):\n",
    "                    output += self._conv_forward(inputs[i],weights[i])\n",
    "                else:\n",
    "                    output += self._conv_forward(inputs[i],weights[i])\n",
    "            output = self.quantize_outputs(output)\n",
    "        \n",
    "#             output_ = self._conv_forward(input, weight)\n",
    "#             print(torch.var(output), torch.var(output_))\n",
    "\n",
    "#         self.groups = ((self.kernel_size[0]*self.kernel_size[1]*self.in_channels) // 256) + 1\n",
    "#         print(self.groups)\n",
    "#         if(self.groups != 1):\n",
    "#             weight = weight.reshape(self.groups,self.out_channels//self.groups, self.in_channels, weight.size()[2], weight.size()[3])\n",
    "#         output =  self._conv_forward(input, weight)\n",
    "#         output = self.quantize_outputs(output)\n",
    "#         print(output.size())\n",
    "#         if(self.groups != 1):\n",
    "#             output = torch.sum(output, 2)\n",
    "        else:\n",
    "            output = self._conv_forward(input, weight)\n",
    "            output = self.quantize_outputs(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter:\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = ProgressMeter._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def print2(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    @classmethod\n",
    "    def _get_batch_fmtstr(cls, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size=3, stride=1, padding=2), \n",
    "#             DIMAConv2d(in_channels = 3, out_channels = 64, kernel_size=3, stride=1, padding=2, sigma_D = sigma_D, layer_index = 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "#             nn.Conv2d(in_channels = 64, out_channels = 192, kernel_size=3, padding=2),\n",
    "            DIMAConv2d(in_channels = 64, out_channels = 192, kernel_size=3, padding=2,sigma_D = sigma_D, layer_index = 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        self.nclass = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_features = self.features(x)\n",
    "        flatten = conv_features.view(conv_features.size(0), -1)\n",
    "        fc = self.fc_layers(flatten)\n",
    "        return fc\n",
    "\n",
    "\n",
    "def get_datasets(*args, **kwargs):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(train=True, transform=transform, *args, **kwargs)\n",
    "    testset = torchvision.datasets.CIFAR10(train=False, transform=transform, *args, **kwargs)\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "def get_dataloaders(trainset, testset, batch_size=100, num_worker=4):\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_worker)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_worker)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def get_model(model_src_path, device='cpu'):\n",
    "    model = Net(num_classes=10)\n",
    "    state_dict = torch.load(model_src_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        if type(output) is tuple:\n",
    "            _, _, output = output\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res, pred[0, :]\n",
    "\n",
    "\n",
    "def eval_single_batch_compute(x, y, model):\n",
    "    output = model(x)\n",
    "    accs, predictions = accuracy(output, y, topk=(1,))\n",
    "    acc = accs[0]\n",
    "    return acc, predictions\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, print_acc=False, device='cpu', log_update_feq=20):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(dataloader),\n",
    "        [top1],\n",
    "        prefix='Evaluating Batch'\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            x, y = data\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            n_data = y.size(0)\n",
    "\n",
    "            acc, predictions = eval_single_batch_compute(x, y, model)\n",
    "\n",
    "            top1.update(acc.item(), n_data)\n",
    "            if idx % log_update_feq == log_update_feq - 1:\n",
    "                progress.print2(idx + 1)\n",
    "#         print('asd')\n",
    "        if print_acc:\n",
    "            print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print('using device:',device)\n",
    "    trainset, testset = get_datasets(root='./data', download=True)\n",
    "    _, testloader = get_dataloaders(trainset, testset, batch_size=100, num_worker=16)\n",
    "\n",
    "    model_src_path = '/Users/Alkin/Documents/ECE498/model.tar' # todo you need to set the path to downloaded model !!\n",
    "    model = get_model(model_src_path, device)\n",
    "    \n",
    "    \n",
    "    eval_model(model, testloader, print_acc=True, device=device)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tUnexpected key(s) in state_dict: \"features.3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-509680e39e69>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mmodel_src_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/Users/Alkin/Documents/ECE498/model.tar'\u001b[0m \u001b[1;31m# todo you need to set the path to downloaded model !!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_src_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-509680e39e69>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(model_src_path, device)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_src_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1052\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tUnexpected key(s) in state_dict: \"features.3.bias\". "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
